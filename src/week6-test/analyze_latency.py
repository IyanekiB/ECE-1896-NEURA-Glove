"""
Analyze End-to-End Latency from Unity CSV Log

PURPOSE:
Analyze latency breakdown from sensor to Unity rendering to identify bottlenecks.

INPUT:
- latency_log.csv: Generated by Unity's LatencyLogger.cs

OUTPUT:
- Console report with latency statistics (mean, median, p95, p99)
- Latency breakdown by component
- Pass/Fail evaluation against targets

USAGE:
python analyze_latency.py [--file latency_log.csv] [--plot]
"""

import pandas as pd
import numpy as np
import argparse
from pathlib import Path


class LatencyAnalyzer:
    """Analyze end-to-end latency from Unity CSV log"""

    def __init__(self, csv_file='latency_log.csv'):
        self.csv_file = csv_file
        self.df = None

        # Latency targets (milliseconds)
        self.TARGET_TOTAL_MS = 33.0  # Target: 30 FPS (33ms per frame)
        self.ACCEPTABLE_TOTAL_MS = 50.0  # Acceptable: 20 FPS (50ms)

    def load_data(self):
        """Load latency CSV file"""
        print(f"Loading latency data from: {self.csv_file}")

        try:
            self.df = pd.read_csv(self.csv_file)
            print(f"✓ Loaded {len(self.df)} latency measurements")
            return True
        except FileNotFoundError:
            print(f"✗ ERROR: File not found: {self.csv_file}")
            return False
        except Exception as e:
            print(f"✗ ERROR loading file: {e}")
            return False

    def calculate_statistics(self, column):
        """Calculate statistics for a given column"""
        data = self.df[column]

        stats = {
            'mean': data.mean(),
            'median': data.median(),
            'std': data.std(),
            'min': data.min(),
            'max': data.max(),
            'p50': data.quantile(0.50),
            'p95': data.quantile(0.95),
            'p99': data.quantile(0.99)
        }

        return stats

    def analyze(self):
        """Main analysis workflow"""
        print("\n" + "="*70)
        print("  END-TO-END LATENCY ANALYSIS")
        print("="*70 + "\n")

        if not self.load_data():
            return False

        # Check for required columns
        required_cols = ['sensor_to_inference_ms', 'inference_time_ms',
                        'inference_to_send_ms', 'udp_transmission_ms',
                        'unity_processing_ms', 'total_latency_ms']

        missing = [col for col in required_cols if col not in self.df.columns]
        if missing:
            print(f"✗ ERROR: Missing columns in CSV: {missing}")
            return False

        # Calculate statistics for each component
        print("LATENCY BREAKDOWN BY COMPONENT")
        print("="*70)

        components = [
            ('Sensor → Inference', 'sensor_to_inference_ms'),
            ('Inference Time', 'inference_time_ms'),
            ('Inference → Send', 'inference_to_send_ms'),
            ('UDP Transmission', 'udp_transmission_ms'),
            ('Unity Processing', 'unity_processing_ms'),
            ('TOTAL LATENCY', 'total_latency_ms')
        ]

        print(f"{'Component':<25} {'Mean':<10} {'Median':<10} {'P95':<10} {'P99':<10} {'Max':<10}")
        print("-" * 70)

        stats_summary = {}

        for name, col in components:
            stats = self.calculate_statistics(col)
            stats_summary[name] = stats

            print(f"{name:<25} "
                  f"{stats['mean']:>7.2f} ms "
                  f"{stats['median']:>7.2f} ms "
                  f"{stats['p95']:>7.2f} ms "
                  f"{stats['p99']:>7.2f} ms "
                  f"{stats['max']:>7.2f} ms")

        # Bottleneck identification
        print("\n" + "="*70)
        print("  BOTTLENECK ANALYSIS")
        print("="*70)

        # Calculate average contribution of each component
        total_mean = stats_summary['TOTAL LATENCY']['mean']

        bottlenecks = []
        for name, col in components[:-1]:  # Exclude total
            stats = stats_summary[name]
            contribution = (stats['mean'] / total_mean) * 100
            bottlenecks.append((name, stats['mean'], contribution))

        bottlenecks.sort(key=lambda x: x[1], reverse=True)

        print("\nComponent Contribution (by average latency):")
        for name, latency, percentage in bottlenecks:
            bar_length = int(percentage / 2)  # Scale for display
            bar = "█" * bar_length
            print(f"  {name:<25} {latency:>6.2f} ms ({percentage:>5.1f}%) {bar}")

        # Pass/Fail evaluation
        print("\n" + "="*70)
        print("  PERFORMANCE EVALUATION")
        print("="*70)

        total_stats = stats_summary['TOTAL LATENCY']

        print(f"\nTotal End-to-End Latency:")
        print(f"  Mean:   {total_stats['mean']:.2f} ms")
        print(f"  Median: {total_stats['median']:.2f} ms")
        print(f"  P95:    {total_stats['p95']:.2f} ms")
        print(f"  P99:    {total_stats['p99']:.2f} ms")
        print(f"  Max:    {total_stats['max']:.2f} ms")

        print(f"\nTargets:")
        print(f"  Target (30 FPS):     < {self.TARGET_TOTAL_MS:.1f} ms")
        print(f"  Acceptable (20 FPS): < {self.ACCEPTABLE_TOTAL_MS:.1f} ms")

        print(f"\nResults:")

        # Evaluate based on mean latency
        mean_latency = total_stats['mean']
        p95_latency = total_stats['p95']

        passed_target = mean_latency < self.TARGET_TOTAL_MS
        passed_acceptable = mean_latency < self.ACCEPTABLE_TOTAL_MS

        if passed_target:
            print(f"  ✓ EXCELLENT: Mean latency {mean_latency:.1f} ms < {self.TARGET_TOTAL_MS:.1f} ms")
        elif passed_acceptable:
            print(f"  ⚠ ACCEPTABLE: Mean latency {mean_latency:.1f} ms < {self.ACCEPTABLE_TOTAL_MS:.1f} ms")
        else:
            print(f"  ✗ POOR: Mean latency {mean_latency:.1f} ms >= {self.ACCEPTABLE_TOTAL_MS:.1f} ms")

        # P95 evaluation
        passed_p95 = p95_latency < self.ACCEPTABLE_TOTAL_MS
        if passed_p95:
            print(f"  ✓ P95 latency {p95_latency:.1f} ms < {self.ACCEPTABLE_TOTAL_MS:.1f} ms")
        else:
            print(f"  ✗ P95 latency {p95_latency:.1f} ms >= {self.ACCEPTABLE_TOTAL_MS:.1f} ms")

        # Frame rate estimate
        achievable_fps = 1000.0 / mean_latency
        print(f"\nEstimated Achievable Frame Rate: {achievable_fps:.1f} FPS")

        print("\n" + "="*70)

        # Latency over time analysis
        self.analyze_stability()

        return True

    def analyze_stability(self):
        """Analyze latency stability over time"""
        print("\n" + "="*70)
        print("  LATENCY STABILITY OVER TIME")
        print("="*70)

        total_latency = self.df['total_latency_ms']

        # Divide into time windows (10% each)
        window_count = 10
        window_size = len(total_latency) // window_count

        print(f"\nAnalyzing {window_count} time windows ({window_size} frames each):\n")

        print(f"{'Window':<10} {'Mean (ms)':<12} {'Std Dev':<12} {'Trend'}")
        print("-" * 50)

        window_means = []
        for i in range(window_count):
            start = i * window_size
            end = start + window_size if i < window_count - 1 else len(total_latency)

            window_data = total_latency[start:end]
            mean = window_data.mean()
            std = window_data.std()

            window_means.append(mean)

            # Determine trend
            if i == 0:
                trend = "-"
            else:
                diff = mean - window_means[i-1]
                if abs(diff) < 1.0:
                    trend = "→ Stable"
                elif diff > 0:
                    trend = f"↑ +{diff:.1f} ms"
                else:
                    trend = f"↓ {diff:.1f} ms"

            print(f"Window {i+1:<3}  {mean:>8.2f} ms   {std:>8.2f} ms   {trend}")

        # Overall stability assessment
        overall_std = total_latency.std()
        mean_of_means = np.mean(window_means)
        std_of_means = np.std(window_means)

        print(f"\nOverall Stability:")
        print(f"  Latency Std Dev (all frames): {overall_std:.2f} ms")
        print(f"  Window Mean Std Dev:          {std_of_means:.2f} ms")

        if std_of_means < 2.0:
            print(f"  ✓ STABLE: Low variance across time windows")
        elif std_of_means < 5.0:
            print(f"  ⚠ MODERATE: Some variance across time windows")
        else:
            print(f"  ✗ UNSTABLE: High variance across time windows")

    def plot_latency(self):
        """Generate latency plots (optional, requires matplotlib)"""
        try:
            import matplotlib.pyplot as plt
        except ImportError:
            print("⚠ Matplotlib not installed - skipping plots")
            return

        print("\nGenerating latency plots...")

        fig, axes = plt.subplots(2, 1, figsize=(12, 8))

        # Plot 1: Total latency over time
        axes[0].plot(self.df['frame'], self.df['total_latency_ms'], alpha=0.7, linewidth=0.5)
        axes[0].axhline(y=self.TARGET_TOTAL_MS, color='g', linestyle='--', label=f'Target ({self.TARGET_TOTAL_MS}ms)')
        axes[0].axhline(y=self.ACCEPTABLE_TOTAL_MS, color='r', linestyle='--', label=f'Acceptable ({self.ACCEPTABLE_TOTAL_MS}ms)')
        axes[0].set_xlabel('Frame')
        axes[0].set_ylabel('Latency (ms)')
        axes[0].set_title('Total End-to-End Latency Over Time')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)

        # Plot 2: Latency component breakdown (stacked area)
        components = ['sensor_to_inference_ms', 'inference_time_ms', 'inference_to_send_ms',
                     'udp_transmission_ms', 'unity_processing_ms']
        labels = ['Sensor→Inference', 'Inference', 'Inference→Send', 'UDP', 'Unity']

        axes[1].stackplot(self.df['frame'],
                         *[self.df[col] for col in components],
                         labels=labels, alpha=0.8)
        axes[1].set_xlabel('Frame')
        axes[1].set_ylabel('Latency (ms)')
        axes[1].set_title('Latency Breakdown by Component')
        axes[1].legend(loc='upper right')
        axes[1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig('latency_analysis.png', dpi=150)
        print("✓ Saved plot: latency_analysis.png")
        plt.show()


def main():
    parser = argparse.ArgumentParser(description='Analyze end-to-end latency from Unity log')
    parser.add_argument('--file', default='latency_log.csv',
                       help='Latency CSV file from Unity')
    parser.add_argument('--plot', action='store_true',
                       help='Generate plots (requires matplotlib)')

    args = parser.parse_args()

    analyzer = LatencyAnalyzer(args.file)
    success = analyzer.analyze()

    if success and args.plot:
        analyzer.plot_latency()

    return 0 if success else 1


if __name__ == "__main__":
    exit(main())
